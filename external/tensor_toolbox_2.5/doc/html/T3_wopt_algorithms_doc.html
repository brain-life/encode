
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML was auto-generated from MATLAB code.
To make changes, update the MATLAB code and republish this document.
      --><title>Weighted optimization for CP tensor decomposition with incomplete data</title><meta name="generator" content="MATLAB 7.13"><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2012-02-01"><meta name="DC.source" content="T3_wopt_algorithms_doc.m"><style type="text/css">

body {
  background-color: white;
  margin:10px;
}

h1 {
  color: #990000; 
  font-size: x-large;
}

h2 {
  color: #990000;
  font-size: medium;
}

/* Make the text shrink to fit narrow windows, but not stretch too far in 
wide windows. */ 
p,h1,h2,div.content div {
  max-width: 600px;
  /* Hack for IE6 */
  width: auto !important; width: 600px;
}

pre.codeinput {
  background: #EEEEEE;
  padding: 10px;
}
@media print {
  pre.codeinput {word-wrap:break-word; width:100%;}
} 

span.keyword {color: #0000FF}
span.comment {color: #228B22}
span.string {color: #A020F0}
span.untermstring {color: #B20000}
span.syscmd {color: #B28C00}

pre.codeoutput {
  color: #666666;
  padding: 10px;
}

pre.error {
  color: red;
}

p.footer {
  text-align: right;
  font-size: xx-small;
  font-weight: lighter;
  font-style: italic;
  color: gray;
}

  </style></head><body><div class="content"><h1>Weighted optimization for CP tensor decomposition with incomplete data</h1><!--introduction--><p>We explain how to use <tt>cp_wopt</tt> with the POBLANO toolbox.</p><!--/introduction--><h2>Contents</h2><div><ul><li><a href="#1">Create an example problem with missing data.</a></li><li><a href="#2">Create initial guess using 'nvecs'</a></li><li><a href="#3">Set up the optimization parameters</a></li><li><a href="#4">Call the <tt>cp_wopt</tt> method</a></li><li><a href="#5">Check the output</a></li><li><a href="#6">Evaluate the output</a></li><li><a href="#7">Create a SPARSE example problem with missing data.</a></li><li><a href="#8">Create initial guess using 'nvecs'</a></li><li><a href="#9">Set up the optimization parameters</a></li><li><a href="#10">Call the <tt>cp_wopt</tt> method</a></li><li><a href="#11">Check the output</a></li><li><a href="#12">Evaluate the output</a></li></ul></div><h2>Create an example problem with missing data.<a name="1"></a></h2><p>Here we have 25% missing data and 10% noise.</p><pre class="codeinput">R = 2;
info = create_problem(<span class="string">'Size'</span>, [15 10 5], <span class="string">'Num_Factors'</span>, R, <span class="keyword">...</span>
    <span class="string">'M'</span>, 0.25, <span class="string">'Noise'</span>, 0.10);
X = info.Data;
P = info.Pattern;
M_true= info.Soln;
</pre><h2>Create initial guess using 'nvecs'<a name="2"></a></h2><pre class="codeinput">M_init = create_guess(<span class="string">'Data'</span>, X, <span class="string">'Num_Factors'</span>, R, <span class="keyword">...</span>
    <span class="string">'Factor_Generator'</span>, <span class="string">'nvecs'</span>);
</pre><h2>Set up the optimization parameters<a name="3"></a></h2><p>It's genearlly a good idea to consider the parameters of the optimization method. The default options may be either too stringent or not stringent enough. The most important options to consider are detailed here.</p><pre class="codeinput"><span class="comment">% Get the defaults</span>
ncg_opts = ncg(<span class="string">'defaults'</span>);
<span class="comment">% Tighten the stop tolerance (norm of gradient). This is often too large.</span>
ncg_opts.StopTol = 1.0e-6;
<span class="comment">% Tighten relative change in function value tolearnce. This is often too large.</span>
ncg_opts.RelFuncTol = 1.0e-20;
<span class="comment">% Increase the number of iterations.</span>
ncg_opts.MaxIters = 10^4;
<span class="comment">% Only display every 10th iteration</span>
ncg_opts.DisplayIters = 10;
<span class="comment">% Display the final set of options</span>
ncg_opts
</pre><pre class="codeoutput">
ncg_opts = 

                   Display: 'iter'
              DisplayIters: 10
           LineSearch_ftol: 1.0000e-004
           LineSearch_gtol: 0.0100
    LineSearch_initialstep: 1
         LineSearch_maxfev: 20
         LineSearch_method: 'more-thuente'
         LineSearch_stpmax: 1.0000e+015
         LineSearch_stpmin: 1.0000e-015
           LineSearch_xtol: 1.0000e-015
              MaxFuncEvals: 10000
                  MaxIters: 10000
                RelFuncTol: 1.0000e-020
              RestartIters: 20
                 RestartNW: 0
              RestartNWTol: 0.1000
                   StopTol: 1.0000e-006
                 TraceFunc: 0
            TraceFuncEvals: 0
                 TraceGrad: 0
             TraceGradNorm: 0
              TraceRelFunc: 0
                    TraceX: 0
                    Update: 'PR'

</pre><h2>Call the <tt>cp_wopt</tt> method<a name="4"></a></h2><p>Here is an example call to the cp_opt method. By default, each iteration prints the least squares fit function value (being minimized) and the norm of the gradient. The meaning of any line search warnings can be checked via <a href="matlab:doc('cvsrch')">doc cvsrch</a>.</p><pre class="codeinput">[M,~,output] = cp_wopt(X, P, R, <span class="string">'init'</span>, M_init, <span class="keyword">...</span>
    <span class="string">'alg'</span>, <span class="string">'ncg'</span>, <span class="string">'alg_options'</span>, ncg_opts);
</pre><pre class="codeoutput"> Iter  FuncEvals       F(X)          ||G(X)||/N        
------ --------- ---------------- ----------------
     0         1      38.81030020       0.21627555
    10        43       0.48416976       0.01540206
    20        68       0.41476299       0.00074832
    30        88       0.41463604       0.00005718
    40       108       0.41463533       0.00000281
    46       120       0.41463533       0.00000084
</pre><h2>Check the output<a name="5"></a></h2><p>It's important to check the output of the optimization method. In particular, it's worthwhile to check the exit flag. A zero (0) indicates successful termination with the gradient smaller than the specified StopTol, and a three (3) indicates a successful termination where the change in function value is less than RelFuncTol. The meaning of any other flags can be checked via <a href="matlab:doc('poblano_params')">doc poblano_params</a>.</p><pre class="codeinput">exitflag = output.ExitFlag
</pre><pre class="codeoutput">
exitflag =

     0

</pre><h2>Evaluate the output<a name="6"></a></h2><p>We can "score" the similarity of the model computed by CP and compare that with the truth. The <tt>score</tt> function on ktensor's gives a score in [0,1]  with 1 indicating a perfect match. Because we have noise, we do not expect the fit to be perfect. See <a href="matlab:doc('ktensor/score')">doc score</a> for more details.</p><pre class="codeinput">scr = score(M,M_true)
</pre><pre class="codeoutput">
scr =

    0.9946

</pre><h2>Create a SPARSE example problem with missing data.<a name="7"></a></h2><p>Here we have 95% missing data and 10% noise.</p><pre class="codeinput">R = 2;
info = create_problem(<span class="string">'Size'</span>, [150 100 50], <span class="string">'Num_Factors'</span>, R, <span class="keyword">...</span>
    <span class="string">'M'</span>, 0.95, <span class="string">'Sparse_M'</span>, true, <span class="string">'Noise'</span>, 0.10);
X = info.Data;
P = info.Pattern;
M_true= info.Soln;
</pre><h2>Create initial guess using 'nvecs'<a name="8"></a></h2><pre class="codeinput">M_init = create_guess(<span class="string">'Data'</span>, X, <span class="string">'Num_Factors'</span>, R, <span class="keyword">...</span>
    <span class="string">'Factor_Generator'</span>, <span class="string">'nvecs'</span>);
</pre><h2>Set up the optimization parameters<a name="9"></a></h2><p>It's genearlly a good idea to consider the parameters of the optimization method. The default options may be either too stringent or not stringent enough. The most important options to consider are detailed here.</p><pre class="codeinput"><span class="comment">% Get the defaults</span>
ncg_opts = ncg(<span class="string">'defaults'</span>);
<span class="comment">% Tighten the stop tolerance (norm of gradient). This is often too large.</span>
ncg_opts.StopTol = 1.0e-6;
<span class="comment">% Tighten relative change in function value tolearnce. This is often too large.</span>
ncg_opts.RelFuncTol = 1.0e-20;
<span class="comment">% Increase the number of iterations.</span>
ncg_opts.MaxIters = 10^4;
<span class="comment">% Only display every 10th iteration</span>
ncg_opts.DisplayIters = 10;
<span class="comment">% Display the final set of options</span>
ncg_opts
</pre><pre class="codeoutput">
ncg_opts = 

                   Display: 'iter'
              DisplayIters: 10
           LineSearch_ftol: 1.0000e-004
           LineSearch_gtol: 0.0100
    LineSearch_initialstep: 1
         LineSearch_maxfev: 20
         LineSearch_method: 'more-thuente'
         LineSearch_stpmax: 1.0000e+015
         LineSearch_stpmin: 1.0000e-015
           LineSearch_xtol: 1.0000e-015
              MaxFuncEvals: 10000
                  MaxIters: 10000
                RelFuncTol: 1.0000e-020
              RestartIters: 20
                 RestartNW: 0
              RestartNWTol: 0.1000
                   StopTol: 1.0000e-006
                 TraceFunc: 0
            TraceFuncEvals: 0
                 TraceGrad: 0
             TraceGradNorm: 0
              TraceRelFunc: 0
                    TraceX: 0
                    Update: 'PR'

</pre><h2>Call the <tt>cp_wopt</tt> method<a name="10"></a></h2><p>Here is an example call to the cp_opt method. By default, each iteration prints the least squares fit function value (being minimized) and the norm of the gradient. The meaning of any line search warnings can be checked via <a href="matlab:doc('cvsrch')">doc cvsrch</a>.</p><pre class="codeinput">[M,~,output] = cp_wopt(X, P, R, <span class="string">'init'</span>, M_init, <span class="keyword">...</span>
    <span class="string">'alg'</span>, <span class="string">'ncg'</span>, <span class="string">'alg_options'</span>, ncg_opts);
</pre><pre class="codeoutput"> Iter  FuncEvals       F(X)          ||G(X)||/N        
------ --------- ---------------- ----------------
     0         1    1379.93609993       0.02752852
    10        58     132.46275289       0.01902211
    20        90     128.70825353       0.00582068
    30       129     126.41868561       0.01373756
    40       170     119.98598848       0.00928409
    50       207     117.10117490       0.01090024
    60       245     114.67006514       0.00842761
    70       278     113.22261378       0.00612126
    80       318     111.97918998       0.00674140
    90       352     110.85741565       0.00471266
   100       390     110.07596824       0.00504442
   110       424     109.22542801       0.00857797
   120       474      96.41782444       0.02745573
   130       521      64.42518274       0.03921470
   140       574      29.96112250       0.04534468
   150       617      14.85584990       0.01242338
   160       654      13.54323681       0.00849761
   170       683      13.42992225       0.00257693
   180       708      13.38722877       0.00084105
   190       729      13.38200130       0.00078035
   200       749      13.38058904       0.00038853
   210       769      13.38020522       0.00021774
   220       789      13.38001729       0.00008396
   230       809      13.37997848       0.00007880
   240       829      13.37996288       0.00003219
   250       849      13.37995884       0.00002386
   260       869      13.37995720       0.00001138
   270       889      13.37995698       0.00000356
   280       909      13.37995692       0.00000097
</pre><h2>Check the output<a name="11"></a></h2><p>It's important to check the output of the optimization method. In particular, it's worthwhile to check the exit flag. A zero (0) indicates successful termination with the gradient smaller than the specified StopTol, and a three (3) indicates a successful termination where the change in function value is less than RelFuncTol. The meaning of any other flags can be checked via <a href="matlab:doc('poblano_params')">doc poblano_params</a>.</p><pre class="codeinput">exitflag = output.ExitFlag
</pre><pre class="codeoutput">
exitflag =

     0

</pre><h2>Evaluate the output<a name="12"></a></h2><p>We can "score" the similarity of the model computed by CP and compare that with the truth. The <tt>score</tt> function on ktensor's gives a score in [0,1]  with 1 indicating a perfect match. Because we have noise, we do not expect the fit to be perfect. See <a href="matlab:doc('ktensor/score')">doc score</a> for more details.</p><pre class="codeinput">scr = score(M,M_true)
</pre><pre class="codeoutput">
scr =

    0.9989

</pre><p class="footer"><br>
      Published with MATLAB&reg; 7.13<br></p></div><!--
##### SOURCE BEGIN #####
%% Weighted optimization for CP tensor decomposition with incomplete data
% We explain how to use |cp_wopt| with the POBLANO toolbox. 

%% Create an example problem with missing data. 
% Here we have 25% missing data and 10% noise.   
R = 2;
info = create_problem('Size', [15 10 5], 'Num_Factors', R, ...
    'M', 0.25, 'Noise', 0.10);
X = info.Data;
P = info.Pattern;
M_true= info.Soln;

%% Create initial guess using 'nvecs'
M_init = create_guess('Data', X, 'Num_Factors', R, ...
    'Factor_Generator', 'nvecs');


%% Set up the optimization parameters
% It's genearlly a good idea to consider the parameters of the optimization
% method. The default options may be either too stringent or not stringent
% enough. The most important options to consider are detailed here. 

% Get the defaults
ncg_opts = ncg('defaults');
% Tighten the stop tolerance (norm of gradient). This is often too large.
ncg_opts.StopTol = 1.0e-6;
% Tighten relative change in function value tolearnce. This is often too large.
ncg_opts.RelFuncTol = 1.0e-20;
% Increase the number of iterations. 
ncg_opts.MaxIters = 10^4;
% Only display every 10th iteration
ncg_opts.DisplayIters = 10;
% Display the final set of options
ncg_opts

%% Call the |cp_wopt| method
% Here is an example call to the cp_opt method. By default, each iteration
% prints the least squares fit function value (being minimized) and the
% norm of the gradient. The meaning of any line search warnings
% can be checked via <matlab:doc('cvsrch') doc cvsrch>.
[M,~,output] = cp_wopt(X, P, R, 'init', M_init, ...
    'alg', 'ncg', 'alg_options', ncg_opts);

%% Check the output
% It's important to check the output of the optimization method. In
% particular, it's worthwhile to check the exit flag. 
% A zero (0) indicates successful termination with the gradient smaller
% than the specified StopTol, and a three (3) indicates a successful
% termination where the change in function value is less than RelFuncTol.
% The meaning of any other flags can be checked via 
% <matlab:doc('poblano_params') doc poblano_params>. 
exitflag = output.ExitFlag


%% Evaluate the output
% We can "score" the similarity of the model computed by CP and compare
% that with the truth. The |score| function on ktensor's gives a score in
% [0,1]  with 1 indicating a perfect match. Because we have noise, we do
% not expect the fit to be perfect. See <matlab:doc('ktensor/score') doc
% score> for more details.
scr = score(M,M_true)

%% Create a SPARSE example problem with missing data. 
% Here we have 95% missing data and 10% noise.   
R = 2;
info = create_problem('Size', [150 100 50], 'Num_Factors', R, ...
    'M', 0.95, 'Sparse_M', true, 'Noise', 0.10);
X = info.Data;
P = info.Pattern;
M_true= info.Soln;

%% Create initial guess using 'nvecs'
M_init = create_guess('Data', X, 'Num_Factors', R, ...
    'Factor_Generator', 'nvecs');


%% Set up the optimization parameters
% It's genearlly a good idea to consider the parameters of the optimization
% method. The default options may be either too stringent or not stringent
% enough. The most important options to consider are detailed here. 

% Get the defaults
ncg_opts = ncg('defaults');
% Tighten the stop tolerance (norm of gradient). This is often too large.
ncg_opts.StopTol = 1.0e-6;
% Tighten relative change in function value tolearnce. This is often too large.
ncg_opts.RelFuncTol = 1.0e-20;
% Increase the number of iterations. 
ncg_opts.MaxIters = 10^4;
% Only display every 10th iteration
ncg_opts.DisplayIters = 10;
% Display the final set of options
ncg_opts

%% Call the |cp_wopt| method
% Here is an example call to the cp_opt method. By default, each iteration
% prints the least squares fit function value (being minimized) and the
% norm of the gradient. The meaning of any line search warnings
% can be checked via <matlab:doc('cvsrch') doc cvsrch>.
[M,~,output] = cp_wopt(X, P, R, 'init', M_init, ...
    'alg', 'ncg', 'alg_options', ncg_opts);

%% Check the output
% It's important to check the output of the optimization method. In
% particular, it's worthwhile to check the exit flag. 
% A zero (0) indicates successful termination with the gradient smaller
% than the specified StopTol, and a three (3) indicates a successful
% termination where the change in function value is less than RelFuncTol.
% The meaning of any other flags can be checked via 
% <matlab:doc('poblano_params') doc poblano_params>. 
exitflag = output.ExitFlag


%% Evaluate the output
% We can "score" the similarity of the model computed by CP and compare
% that with the truth. The |score| function on ktensor's gives a score in
% [0,1]  with 1 indicating a perfect match. Because we have noise, we do
% not expect the fit to be perfect. See <matlab:doc('ktensor/score') doc
% score> for more details.
scr = score(M,M_true)


##### SOURCE END #####
--></body></html>